{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "image_list = []\n",
    "label_list = []\n",
    "for filename in glob.glob('D:/TUD/TU_Dresden/WiSe_2021/Thesis_FZJ/data_trial//*.tif'): #assuming gif\n",
    "    im=Image.open(filename)\n",
    "    image_list.append(np.asarray(im))\n",
    "    #label = encoding(file_name = filename)\n",
    "    label = int(filename.split('/')[-1].split('_')[3])\n",
    "    print(label)\n",
    "    label_list.append(label)\n",
    "\n",
    "data_h5 = h5py.File('data_tbc_patches_final.h5', 'w')\n",
    "dset = data_h5.create_dataset('data', data = image_list, chunks=True, compression=\"gzip\",compression_opts=9)\n",
    "lset = data_h5.create_dataset('labels', data = label_list, chunks=True, compression=\"gzip\", compression_opts=9)\n",
    "data_h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "def create_split(data_path, output_name, n=3000):\n",
    "    \"\"\"\n",
    "    data-- created hdf65 data set, which will be split\n",
    "    output-- desired name for out put file\n",
    "    n-- number of validation data points\n",
    "    \"\"\"\n",
    "    data = h5py.File(data_path, 'r')\n",
    "\n",
    "    with h5py.File(output_name, 'w') as out:\n",
    "        indexes = np.arange(data['data'].shape[0])\n",
    "        np.random.shuffle(indexes)\n",
    "        for key in data.keys():\n",
    "            print(key)\n",
    "            feed = np.take(data[key], indexes, axis=0)\n",
    "            out.create_dataset(key, data=feed)\n",
    "        out.create_dataset('data_train', data=out['data'][n:], chunks=True, compression=\"gzip\",compression_opts=9)\n",
    "        #out.create_dataset('data_val', data=out['data'][0:n], chunks=True, compression=\"gzip\",compression_opts=9)\n",
    "        out.create_dataset('train_label', data=out['labels'][n:], chunks=True, compression=\"gzip\",compression_opts=9) \n",
    "        #out.create_dataset('val_label', data=out['labels'][0:n], chunks=True, compression=\"gzip\",compression_opts=9)\n",
    "        print(out.keys())\n",
    "        del out['data'],out['labels']\n",
    "        print(out.keys())\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data\n",
      "labels\n",
      "<KeysViewHDF5 ['data', 'data_train', 'labels', 'train_label']>\n",
      "<KeysViewHDF5 ['data_train', 'train_label']>\n"
     ]
    }
   ],
   "source": [
    "path = 'D:\\TUD\\TU_Dresden\\WiSe_2021\\data_tbc_patches_final.h5'\n",
    "create_split(path, 'final_data.h5', 3000)"
   ]
  }
 ]
}